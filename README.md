# BigData

This repository contains projects structured around the utilization of major data technologies including PostgreSQL, MongoDB, Hadoop/MapReduce, and PySpark, covering various data manipulation and analysis techniques.

# Structure
# Relational DB Model and SQL
Description: Designing a relational database using PostgreSQL and conducting data analysis with SQL.  
Key Tasks:  
-Design a relational database with at least three tables.  
-Query the database to find the oldest books, highest rated language codes, authors with the most ratings, and publishers with books in multiple languages.
# MongoDB and Flask
Description: Creating a data product using MongoDB and Flask.  
Key Tasks:  
-Data cleanup and insertion into a MongoDB database.  
-Extend the database with external API data.  
-Develop a Flask web application that features biographical information and book links for selected authors.  
# Hadoop/MapReduce
Description: Extending Python mapper and reducer functions to analyze text data.  
Key Tasks:  
-Develop a Python code to count bigrams and digrams in text data.  
# Spark
Description: Utilizing PySpark to perform data analysis and manipulation tasks.  
Key Tasks:  
-Analyze purchase data to find top items bought between 9:00 AM and 11:00 PM.  
-Summarize the number of entities by their area description in restaurant data.  
-Compute yearly population growth percentages by region and identify the highest and lowest growth countries.  
-Conduct a word count on a given text file.  
-Determine the number of foreclosures within a 10-mile radius of active restaurants.  

# Setup and Running Instructions
Environment Setup: Ensure you have Python and PySpark installed, along with necessary libraries such as haversine for geographical calculations.  
Running Notebooks: Each notebook can be executed within JupyterHub where datasets are directly accessible.  
Database Configurations: Ensure PostgreSQL and MongoDB are properly set up according to the instructions in their respective assignment folders.  
